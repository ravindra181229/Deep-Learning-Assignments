{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30673a09",
   "metadata": {},
   "source": [
    "# DL_assignment - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad54238a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nANN have a layered architecture and each network node has the capability to process input and forward output to other nodes in the network.\\n\\nArtificial neuron also known as perceptron is the basic unit of the neural network.\\nIn simple terms, it is a mathematical function based on a model of biological neurons. It can also be seen as a simple logic gate with binary outputs. \\nThey are sometimes also called perceptrons.\\ninput,weights,Transfer function, activation function, bias\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "\"\"\"\n",
    "ANN have a layered architecture and each network node has the capability to process input and forward output to other nodes in the network.\n",
    "\n",
    "Artificial neuron also known as perceptron is the basic unit of the neural network.\n",
    "In simple terms, it is a mathematical function based on a model of biological neurons. It can also be seen as a simple logic gate with binary outputs. \n",
    "They are sometimes also called perceptrons.\n",
    "input,weights,Transfer function, activation function, bias\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1ae3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBinary Step Function,\\nLinear Function,\\nSigmoid,\\nTanh,\\nReLU,\\nLeaky ReLU,\\nParameterised ReLU,\\nExponential Linear Unit.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "\"\"\"\n",
    "Binary Step Function,\n",
    "Linear Function,\n",
    "Sigmoid,\n",
    "Tanh,\n",
    "ReLU,\n",
    "Leaky ReLU,\n",
    "Parameterised ReLU,\n",
    "Exponential Linear Unit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c149aae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n3. 1)\\nRosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. \\nIf the result of this addition is larger than a given threshold θ the neuron fires\\n\\n3. 2)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "\"\"\"\n",
    "3. 1)\n",
    "Rosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. \n",
    "If the result of this addition is larger than a given threshold θ the neuron fires\n",
    "\n",
    "3. 2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c1f5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe XOR problem with neural networks can be solved by using Multi-Layer Perceptrons or a neural network architecture with an input layer, hidden layer, and output layer. \\nSo during the forward propagation through the neural networks, the weights get updated to the corresponding layers and the XOR logic gets executed.\\n\\nwe have built a multi layered perceptron with the following weights and it predicts the output of a XOr logical operator.\\nUsing the formulae for AND, NOT and OR gates, we get:\\nh1 = σ((1-x1) + x2) = σ((-1)x1 + x2 + 1)\\nh2 = σ(x1 + (1-x2)) = σ(x1 + (-1)x2 + 1)\\ny = σ(h1 + h2) = σ(h1 + h2 + 0)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "\"\"\"\n",
    "The XOR problem with neural networks can be solved by using Multi-Layer Perceptrons or a neural network architecture with an input layer, hidden layer, and output layer. \n",
    "So during the forward propagation through the neural networks, the weights get updated to the corresponding layers and the XOR logic gets executed.\n",
    "\n",
    "we have built a multi layered perceptron with the following weights and it predicts the output of a XOr logical operator.\n",
    "Using the formulae for AND, NOT and OR gates, we get:\n",
    "h1 = σ((1-x1) + x2) = σ((-1)x1 + x2 + 1)\n",
    "h2 = σ(x1 + (1-x2)) = σ(x1 + (-1)x2 + 1)\n",
    "y = σ(h1 + h2) = σ(h1 + h2 + 0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e29e442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe term \"Artificial neural network\" refers to a biologically inspired sub-field of artificial intelligence modeled after the brain. \\nAn Artificial neural network is usually a computational network based on biological neural networks that construct the structure of the human brain.\\n\\nANN is made of three layers namely input layer, output layer, and hidden layer/s. \\nThere must be a connection from the nodes in the input layer with the nodes in the hidden layer and from each hidden layer node with the nodes of the output layer\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "\"\"\"\n",
    "The term \"Artificial neural network\" refers to a biologically inspired sub-field of artificial intelligence modeled after the brain. \n",
    "An Artificial neural network is usually a computational network based on biological neural networks that construct the structure of the human brain.\n",
    "\n",
    "ANN is made of three layers namely input layer, output layer, and hidden layer/s. \n",
    "There must be a connection from the nodes in the input layer with the nodes in the hidden layer and from each hidden layer node with the nodes of the output layer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01bbda3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAn artificial neural network's learning rule or learning process is a method,\\nmathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network.\\n\\nLearning, in artificial neural network, is the method of modifying the weights of connections between the neurons of a specified network. \\nLearning in ANN can be classified into three categories namely supervised learning, unsupervised learning, and reinforcement learning.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "\"\"\"\n",
    "An artificial neural network's learning rule or learning process is a method,\n",
    "mathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network.\n",
    "\n",
    "Learning, in artificial neural network, is the method of modifying the weights of connections between the neurons of a specified network. \n",
    "Learning in ANN can be classified into three categories namely supervised learning, unsupervised learning, and reinforcement learning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24b916fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe backpropagation algorithm works by computing the gradient of the loss function with respect to each weight \\nvia the chain rule, computing the gradient layer by layer, and iterating backward from the last layer to avoid \\nredundant computation of intermediate terms in the chain rule\\n\\nThe biggest disadvantages of backpropagation are: \\nBackpropagation could be rather sensitive to noisy data and irregularity. \\nThe performance of backpropagation relies very heavily on the training data. \\nBackpropagation needs a very large amount of time for training.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "\"\"\"\n",
    "The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight \n",
    "via the chain rule, computing the gradient layer by layer, and iterating backward from the last layer to avoid \n",
    "redundant computation of intermediate terms in the chain rule\n",
    "\n",
    "The biggest disadvantages of backpropagation are: \n",
    "Backpropagation could be rather sensitive to noisy data and irregularity. \n",
    "The performance of backpropagation relies very heavily on the training data. \n",
    "Backpropagation needs a very large amount of time for training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b31ca6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne main part of the algorithm is adjusting the interconnection weights. \\nThis is done using a technique termed as Gradient Descent\\n\\nThe backpropagation algorithm starts with random weights, and the goal is to adjust them to reduce this error until the ANN learns the training data.\\nStandard backpropagation is a gradient descent algorithm in which the network weights are moved along the negative of the gradient of the performance function.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "\"\"\"\n",
    "One main part of the algorithm is adjusting the interconnection weights. \n",
    "This is done using a technique termed as Gradient Descent\n",
    "\n",
    "The backpropagation algorithm starts with random weights, and the goal is to adjust them to reduce this error until the ANN learns the training data.\n",
    "Standard backpropagation is a gradient descent algorithm in which the network weights are moved along the negative of the gradient of the performance function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3713552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: Inputs X, arrive through the preconnected path. \\nStep 2: The input is modeled using true weights W. Weights are usually chosen randomly. \\nStep 3: Calculate the output of each neuron from the input layer to the hidden layer to the output layer.\\n\\n>> A multilayer neural network with appropriate weights has been shown to be able to approximate any input-output function making it an attractive tool for modeling and forecasting\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "\"\"\"\n",
    "Step 1: Inputs X, arrive through the preconnected path. \n",
    "Step 2: The input is modeled using true weights W. Weights are usually chosen randomly. \n",
    "Step 3: Calculate the output of each neuron from the input layer to the hidden layer to the output layer.\n",
    "\n",
    ">> A multilayer neural network with appropriate weights has been shown to be able to approximate any input-output function making it an attractive tool for modeling and forecasting\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "818066b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nArtificial neuron :\\nAn artificial neuron is a connection point in an artificial neural network.\\nANN like the human body's biological neural network, have a layered architecture and each network node \\nhas the capability to process input and forward output to other nodes in the network.\\n\\nMulti-layer perceptron :\\nA multilayer perceptron (MLP) is a feedforward artificial neural network that generates a set of outputs from a set of inputs. \\nAn MLP is characterized by several layers of input nodes connected as a directed graph between the input and output layers\\n\\nDeep learning :\\nDeep learning is a type of machine learning and artificial intelligence (AI) that imitates the way humans gain certain types of knowledge.\\n\\nLearning rate:\\nthe learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "\"\"\"\n",
    "Artificial neuron :\n",
    "An artificial neuron is a connection point in an artificial neural network.\n",
    "ANN like the human body's biological neural network, have a layered architecture and each network node \n",
    "has the capability to process input and forward output to other nodes in the network.\n",
    "\n",
    "Multi-layer perceptron :\n",
    "A multilayer perceptron (MLP) is a feedforward artificial neural network that generates a set of outputs from a set of inputs. \n",
    "An MLP is characterized by several layers of input nodes connected as a directed graph between the input and output layers\n",
    "\n",
    "Deep learning :\n",
    "Deep learning is a type of machine learning and artificial intelligence (AI) that imitates the way humans gain certain types of knowledge.\n",
    "\n",
    "Learning rate:\n",
    "the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70b91ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActivation function vs threshold function :\\nBinary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated\\nThe activation function compares the input value to a threshold value. If the input value is greater than the threshold value, the neuron is activated\\n\\nStep function vs sigmoid function : \\nThe step-function output is y = 1 if x ≥ θ , 0 if x ≤ θ . \\nThe sigmoid function, more commonly used, is asymptotic to 0 and 1 and antisymmetric about (0, 0.5): 1 g ( x ) = , β> 0 1 + e − βx ANNs may be feedforward.\\n\\nSingle layer vs multi-layer perceptron : \\nA Multi Layer Perceptron (MLP) contains one or more hidden layers \\nWhile a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non - linear functions\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q9\n",
    "\"\"\"\n",
    "Activation function vs threshold function :\n",
    "Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated\n",
    "The activation function compares the input value to a threshold value. If the input value is greater than the threshold value, the neuron is activated\n",
    "\n",
    "Step function vs sigmoid function : \n",
    "The step-function output is y = 1 if x ≥ θ , 0 if x ≤ θ . \n",
    "The sigmoid function, more commonly used, is asymptotic to 0 and 1 and antisymmetric about (0, 0.5): 1 g ( x ) = , β> 0 1 + e − βx ANNs may be feedforward.\n",
    "\n",
    "Single layer vs multi-layer perceptron : \n",
    "A Multi Layer Perceptron (MLP) contains one or more hidden layers \n",
    "While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non - linear functions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b06d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
