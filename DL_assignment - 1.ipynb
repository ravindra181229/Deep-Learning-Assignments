{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce77c73a",
   "metadata": {},
   "source": [
    "# DL_assignment - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25951fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe linear combiner or the summing junction adds all the products of the synapses and parameters.\\nA threshold activation function results in an output signal only when an input signal exceeding a specific threshold value comes as an input.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "\"\"\"\n",
    "The linear combiner or the summing junction adds all the products of the synapses and parameters.\n",
    "A threshold activation function results in an output signal only when an input signal exceeding a specific threshold value comes as an input.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24cfa20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA step function is a function like that used by the original Perceptron.\\nThe output is a certain value, A1, if the input sum is above a certain threshold and A0 if the input sum is below a certain threshold. The values used by the Perceptron were A1 = 1 and A0 = 0\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "\"\"\"\n",
    "A step function is a function like that used by the original Perceptron.\n",
    "The output is a certain value, A1, if the input sum is above a certain threshold and A0 if the input sum is below a certain threshold. The values used by the Perceptron were A1 = 1 and A0 = 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b15899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe McCulloch-Pitts neural model, which was the earliest ANN model, has only two types of inputs — Excitatory and Inhibitory.\\nThe excitatory inputs have weights of positive magnitude and the inhibitory weights have weights of negative magnitude. \\nThe inputs of the McCulloch-Pitts neuron could be either 0 or 1\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "\"\"\"\n",
    "The McCulloch-Pitts neural model, which was the earliest ANN model, has only two types of inputs — Excitatory and Inhibitory.\n",
    "The excitatory inputs have weights of positive magnitude and the inhibitory weights have weights of negative magnitude. \n",
    "The inputs of the McCulloch-Pitts neuron could be either 0 or 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bae982a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nADALINE is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers\\ni.e. its activation function is the sign function. The three-layer network uses memistors\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "\"\"\"\n",
    "ADALINE is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers\n",
    "i.e. its activation function is the sign function. The three-layer network uses memistors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5e20e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\none of the challenges with the single-layer perceptrons is that the constraints limit the algorithm's ability to accurately classify data. \\nSpecifically, the primary constraint is that the data must be linearly separable.\\n\\nPerceptrons only represent linearly separable problems. They fail to converge if the training examples are not linearly separable.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "\"\"\"\n",
    "one of the challenges with the single-layer perceptrons is that the constraints limit the algorithm's ability to accurately classify data. \n",
    "Specifically, the primary constraint is that the data must be linearly separable.\n",
    "\n",
    "Perceptrons only represent linearly separable problems. They fail to converge if the training examples are not linearly separable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "799b6119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nClearly not all decision problems are linearly separable: they cannot be solved using a linear decision boundary. Problems like these are termed linearly inseparable.\\n\\nA hidden layer in an ANN is a layer in between input layers and output layers, where artificial neurons take in a set of weighted inputs and produce an output through an activation function.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "\"\"\"\n",
    "Clearly not all decision problems are linearly separable: they cannot be solved using a linear decision boundary. Problems like these are termed linearly inseparable.\n",
    "\n",
    "A hidden layer in an ANN is a layer in between input layers and output layers, where artificial neurons take in a set of weighted inputs and produce an output through an activation function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f53d05d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe XOr problem is that we need to build a Neural Network  to produce the truth table related to the XOr logical operator. \\nThis is a binary classification problem. Hence, supervised learning is a better way to solve it. In this case, we will be using perceptrons\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "\"\"\"\n",
    "The XOr problem is that we need to build a Neural Network  to produce the truth table related to the XOr logical operator. \n",
    "This is a binary classification problem. Hence, supervised learning is a better way to solve it. In this case, we will be using perceptrons\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa87e6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwe have built a multi layered perceptron with the following weights and it predicts the output of a XOr logical operator.\\n\\nUsing the formulae for AND, NOT and OR gates we get:\\nh1 = σ((1-x1) + x2) = σ((-1)x1 + x2 + 1)\\nh2 = σ(x1 + (1-x2)) = σ(x1 + (-1)x2 + 1)\\ny = σ(h1 + h2) = σ(h1 + h2 + 0)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "\"\"\"\n",
    "we have built a multi layered perceptron with the following weights and it predicts the output of a XOr logical operator.\n",
    "\n",
    "Using the formulae for AND, NOT and OR gates we get:\n",
    "h1 = σ((1-x1) + x2) = σ((-1)x1 + x2 + 1)\n",
    "h2 = σ(x1 + (1-x2)) = σ(x1 + (-1)x2 + 1)\n",
    "y = σ(h1 + h2) = σ(h1 + h2 + 0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e53c196d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe output layer is formed when different weights are applied to input nodes and the cumulative effect per node is taken.\\nAfter this, the neurons collectively give the output layer to compute the output signals\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q9\n",
    "\"\"\"\n",
    "The output layer is formed when different weights are applied to input nodes and the cumulative effect per node is taken.\n",
    "After this, the neurons collectively give the output layer to compute the output signals\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8be68176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA neural network consists of three layers. \\nThe first layer is the input layer.\\nIt contains the input neurons that send information to the hidden layer.\\nThe hidden layer performs the computations on input data and transfers the output to the output layer.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q10\n",
    "\"\"\"\n",
    "A neural network consists of three layers. \n",
    "The first layer is the input layer.\n",
    "It contains the input neurons that send information to the hidden layer.\n",
    "The hidden layer performs the computations on input data and transfers the output to the output layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d450d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe backpropagation algorithm performs learning on a multilayer feed-forward neural network. \\nIt iteratively learns a set of weights for prediction of the class label of tuples.\\nA multilayer feed-forward neural network consists of an input layer, one or more hidden layers, and an output layer.\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q11\n",
    "\"\"\"\n",
    "The backpropagation algorithm performs learning on a multilayer feed-forward neural network. \n",
    "It iteratively learns a set of weights for prediction of the class label of tuples.\n",
    "A multilayer feed-forward neural network consists of an input layer, one or more hidden layers, and an output layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f39226d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdvantages :\\nStore information on the entire network,\\nThe ability to work with insufficient knowledge,\\nGood falt tolerance,\\nDistributed memory,\\nGradual Corruption,\\nAbility to train machine\\n\\nDisadvantages :\\nBlack Box,\\nDuration of Development,\\nAmount of Data,\\nComputationally Expensive.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q12\n",
    "\"\"\"\n",
    "Advantages :\n",
    "Store information on the entire network,\n",
    "The ability to work with insufficient knowledge,\n",
    "Good falt tolerance,\n",
    "Distributed memory,\n",
    "Gradual Corruption,\n",
    "Ability to train machine\n",
    "\n",
    "Disadvantages :\n",
    "Black Box,\n",
    "Duration of Development,\n",
    "Amount of Data,\n",
    "Computationally Expensive.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2cb1b4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGradient descent :\\nGradient descent is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. \\nThis method is commonly used in deep learning(DL) to minimise a cost/loss function\\n\\nSingle-layer feed forward ANN : \\na single layer FNN consisting of I input neurons, m hidden neurons, and one output neuron together with its adjustable parameters\\ni.e., weights and biases\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q13\n",
    "\"\"\"\n",
    "Gradient descent :\n",
    "Gradient descent is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. \n",
    "This method is commonly used in deep learning(DL) to minimise a cost/loss function\n",
    "\n",
    "Single-layer feed forward ANN : \n",
    "a single layer FNN consisting of I input neurons, m hidden neurons, and one output neuron together with its adjustable parameters\n",
    "i.e., weights and biases\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6cfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
